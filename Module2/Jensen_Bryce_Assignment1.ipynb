{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Run - No Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 154us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 764\n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change 1 - Reducing nodes to a quarter of original (32 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 1.7945 - accuracy: 0.4626 - val_loss: 1.2099 - val_accuracy: 0.7001\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.8772 - accuracy: 0.7616 - val_loss: 0.6284 - val_accuracy: 0.8272\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.5659 - accuracy: 0.8438 - val_loss: 0.4663 - val_accuracy: 0.8764\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.4558 - accuracy: 0.8735 - val_loss: 0.3973 - val_accuracy: 0.8917\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.4017 - accuracy: 0.8864 - val_loss: 0.3595 - val_accuracy: 0.9011\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3694 - accuracy: 0.8947 - val_loss: 0.3373 - val_accuracy: 0.9047\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3473 - accuracy: 0.9005 - val_loss: 0.3194 - val_accuracy: 0.9098\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3308 - accuracy: 0.9054 - val_loss: 0.3055 - val_accuracy: 0.9137\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3178 - accuracy: 0.9088 - val_loss: 0.2952 - val_accuracy: 0.9162\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3069 - accuracy: 0.9119 - val_loss: 0.2874 - val_accuracy: 0.9169\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2972 - accuracy: 0.9148 - val_loss: 0.2779 - val_accuracy: 0.9221\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2884 - accuracy: 0.9176 - val_loss: 0.2708 - val_accuracy: 0.9236\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2806 - accuracy: 0.9195 - val_loss: 0.2661 - val_accuracy: 0.9217\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2737 - accuracy: 0.9215 - val_loss: 0.2583 - val_accuracy: 0.9254\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2668 - accuracy: 0.9238 - val_loss: 0.2523 - val_accuracy: 0.9266\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2605 - accuracy: 0.9250 - val_loss: 0.2469 - val_accuracy: 0.9288\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2546 - accuracy: 0.9268 - val_loss: 0.2419 - val_accuracy: 0.9302\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.2489 - accuracy: 0.9288 - val_loss: 0.2371 - val_accuracy: 0.9322\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2435 - accuracy: 0.9295 - val_loss: 0.2339 - val_accuracy: 0.9337\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2384 - accuracy: 0.9317 - val_loss: 0.2287 - val_accuracy: 0.9350\n",
      "10000/10000 [==============================] - 0s 23us/step\n",
      "Test score: 0.23256392626613379\n",
      "Test accuracy: 0.9336000084877014\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 32 \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 764\n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change 2 - reducing nodes to a quarter again (8 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 6,442\n",
      "Trainable params: 6,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 1.9599 - accuracy: 0.3269 - val_loss: 1.5684 - val_accuracy: 0.4834\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 1.2649 - accuracy: 0.5992 - val_loss: 1.0186 - val_accuracy: 0.6651\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.9565 - accuracy: 0.6929 - val_loss: 0.8371 - val_accuracy: 0.7330\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.8168 - accuracy: 0.7511 - val_loss: 0.7274 - val_accuracy: 0.7768\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.7278 - accuracy: 0.7858 - val_loss: 0.6543 - val_accuracy: 0.8029\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.6685 - accuracy: 0.8075 - val_loss: 0.6032 - val_accuracy: 0.8233\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.6229 - accuracy: 0.8223 - val_loss: 0.5628 - val_accuracy: 0.8369\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.5843 - accuracy: 0.8345 - val_loss: 0.5285 - val_accuracy: 0.8468\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.5516 - accuracy: 0.8426 - val_loss: 0.5006 - val_accuracy: 0.8547\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.5224 - accuracy: 0.8527 - val_loss: 0.4753 - val_accuracy: 0.8620\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.4964 - accuracy: 0.8601 - val_loss: 0.4519 - val_accuracy: 0.8697\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.4718 - accuracy: 0.8671 - val_loss: 0.4317 - val_accuracy: 0.8773\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.4497 - accuracy: 0.8737 - val_loss: 0.4109 - val_accuracy: 0.8827\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.4290 - accuracy: 0.8802 - val_loss: 0.3934 - val_accuracy: 0.8882\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.4110 - accuracy: 0.8847 - val_loss: 0.3787 - val_accuracy: 0.8939\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.3961 - accuracy: 0.8890 - val_loss: 0.3667 - val_accuracy: 0.8960\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.3833 - accuracy: 0.8926 - val_loss: 0.3569 - val_accuracy: 0.8983\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.3725 - accuracy: 0.8954 - val_loss: 0.3489 - val_accuracy: 0.9020\n",
      "Epoch 19/20\n",
      " 3840/48000 [=>............................] - ETA: 4s - loss: 0.3455 - accuracy: 0.8995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.183757). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.3636 - accuracy: 0.8975 - val_loss: 0.3423 - val_accuracy: 0.9035\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.3557 - accuracy: 0.9003 - val_loss: 0.3366 - val_accuracy: 0.9035\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Test score: 0.34400188852548597\n",
      "Test accuracy: 0.8988999724388123\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 8 \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 764\n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change 3 - 4x the original number of nodes (512 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 24s 494us/step - loss: 1.2538 - accuracy: 0.7346 - val_loss: 0.6053 - val_accuracy: 0.8694\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 26s 538us/step - loss: 0.5101 - accuracy: 0.8736 - val_loss: 0.4034 - val_accuracy: 0.8934\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 28s 574us/step - loss: 0.3948 - accuracy: 0.8944 - val_loss: 0.3430 - val_accuracy: 0.9067\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 28s 578us/step - loss: 0.3465 - accuracy: 0.9047 - val_loss: 0.3122 - val_accuracy: 0.9130\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 25s 520us/step - loss: 0.3171 - accuracy: 0.9116 - val_loss: 0.2903 - val_accuracy: 0.9187\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 27s 560us/step - loss: 0.2960 - accuracy: 0.9174 - val_loss: 0.2746 - val_accuracy: 0.9227\n",
      "Epoch 7/20\n",
      "  256/48000 [..............................] - ETA: 49s - loss: 0.3262 - accuracy: 0.9258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.104031). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 25s 519us/step - loss: 0.2791 - accuracy: 0.9218 - val_loss: 0.2605 - val_accuracy: 0.9260\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 28s 593us/step - loss: 0.2648 - accuracy: 0.9264 - val_loss: 0.2490 - val_accuracy: 0.9301\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 30s 631us/step - loss: 0.2530 - accuracy: 0.9288 - val_loss: 0.2392 - val_accuracy: 0.9327\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 24s 491us/step - loss: 0.2421 - accuracy: 0.9321 - val_loss: 0.2321 - val_accuracy: 0.9353\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 12s 244us/step - loss: 0.2323 - accuracy: 0.9347 - val_loss: 0.2228 - val_accuracy: 0.9373\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 12s 245us/step - loss: 0.2232 - accuracy: 0.9372 - val_loss: 0.2158 - val_accuracy: 0.9406\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 13s 261us/step - loss: 0.2150 - accuracy: 0.9396 - val_loss: 0.2117 - val_accuracy: 0.9407\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 13s 267us/step - loss: 0.2076 - accuracy: 0.9421 - val_loss: 0.2035 - val_accuracy: 0.9433\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 0.2004 - accuracy: 0.9441 - val_loss: 0.1967 - val_accuracy: 0.9449\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 0.1938 - accuracy: 0.9457 - val_loss: 0.1916 - val_accuracy: 0.9463\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 13s 276us/step - loss: 0.1876 - accuracy: 0.9470 - val_loss: 0.1865 - val_accuracy: 0.9477\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 13s 277us/step - loss: 0.1816 - accuracy: 0.9487 - val_loss: 0.1822 - val_accuracy: 0.9484\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 12s 260us/step - loss: 0.1762 - accuracy: 0.9499 - val_loss: 0.1784 - val_accuracy: 0.9502\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 0.1711 - accuracy: 0.9516 - val_loss: 0.1736 - val_accuracy: 0.9511\n",
      "10000/10000 [==============================] - 4s 419us/step\n",
      "Test score: 0.17235374244600535\n",
      "Test accuracy: 0.9509000182151794\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 512 \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 764\n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Original Accuracy: 94.63%\n",
    "\n",
    "### Change 1 Accuracy: 93.36%\n",
    "\n",
    "### Change 2 Accuracy: 89.89%\n",
    "\n",
    "### Change 3 Accuracy: 95.09%\n",
    "\n",
    "\n",
    "As one can see by the accuracy rates, lowering the number of nodes (hidden neurons) in the code results in a decrease in the accuracy, where increasing them gives an increase in the accuracy rate of the tests. However the differences in accuracy weren't impacted significantly until the number of neurons were drastically reduced, which shows that the dataset wasn't too complex and it likely didn't need as many as it initially had. One thing that I did notice was that the amount of time per step, went up hugely when the number of nodes were raised to 4 times the original amount. Where the original took about 2 seconds per epoch, when it had 512 nodes it went up to about 12-30 seconds per epoch. It took ~10 times the amount of time, but increased less than half a percent for accuracy. Was it worth it?\n",
    "\n",
    "One thing that I've read can significantly increase the accuracy is changing the optimizer. One of the ones I was reading about is called Adam (we used SGD in this example). I ran a test, the results are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change 4 - Optimizer to Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.3567 - accuracy: 0.8990 - val_loss: 0.1709 - val_accuracy: 0.9494\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.1400 - accuracy: 0.9592 - val_loss: 0.1279 - val_accuracy: 0.9619\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.0969 - accuracy: 0.9715 - val_loss: 0.1091 - val_accuracy: 0.9686\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.0718 - accuracy: 0.9785 - val_loss: 0.1058 - val_accuracy: 0.9688\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.0567 - accuracy: 0.9831 - val_loss: 0.0915 - val_accuracy: 0.9722\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.0418 - accuracy: 0.9876 - val_loss: 0.0906 - val_accuracy: 0.9744\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.0344 - accuracy: 0.9893 - val_loss: 0.0894 - val_accuracy: 0.9741\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0885 - val_accuracy: 0.9750\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.0215 - accuracy: 0.9937 - val_loss: 0.0902 - val_accuracy: 0.9766\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.1006 - val_accuracy: 0.9735\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.1001 - val_accuracy: 0.9753\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.1077 - val_accuracy: 0.9756\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.1056 - val_accuracy: 0.9762\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.1117 - val_accuracy: 0.9758\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.1152 - val_accuracy: 0.9737\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.1114 - val_accuracy: 0.9758\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.1330 - val_accuracy: 0.9752\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.1212 - val_accuracy: 0.9769\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.1226 - val_accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.1304 - val_accuracy: 0.9759\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score: 0.1100124992236484\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import Adam \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 764\n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change 4 Results\n",
    "With the only change from the original being the optimizer changed from SGD to Adam, we got a score over 3% higher than the original code. It did take a bit longer though, so that should be taken into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
